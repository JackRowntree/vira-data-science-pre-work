{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary \n",
    "\n",
    "Thank you for your interest in Vira Health's data science team!  \n",
    "\n",
    "This jupyter notebook contains instructions for a short task which will give you an insight into some of what Vira Health is working on.   \n",
    "\n",
    "The task is structured around building a simple 1 page dashboard that summarises what you think are the key characteristics of the datasets provided.  \n",
    "\n",
    "Data provided is courtesy of the Study of Women's Health Across the Nation (SWAN) and is publically available from their [website](https://www.swanstudy.org/).      \n",
    "\n",
    "Additional documentation can be found in the ICPSR data repository [here](https://www.icpsr.umich.edu/web/ICPSR/series/00253) and may be helpful to support completion of the task.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data exploration \n",
    "\n",
    "The /data folder includes data from a questionnaire collected at baseline (\"swan_1996_97_baseline.csv\") and two annual follow-up visits (\"swan_1997_99_visit1.csv\", \"swan_1998_00_visit2.csv\").  \n",
    "\n",
    "Documentation for the baseline visit with details of the questionnaire and variables referenced is also included in the /data folder as \"baseline-visit-codebook-PI.pdf\".  \n",
    "\n",
    "As a first step, please load in the data in this python notebook and conduct whatever exploration you need to decide - **\"What are the key characteristics of these datasets?\"**.   \n",
    "\n",
    "To help focus, remember that the overall aim of the task is to **build a simple 1 page dashboard that summarises the key characteristics of the datasets**.    \n",
    "\n",
    "Example exploration could include answering sub-questions such as, what is the size of each sample? how many participants have data in all follow-up visits?  \n",
    "\n",
    "Please include inline code comments or markdown to explain your approach.  \n",
    "\n",
    "Note, this exploration is not expected to be comprehensive, but if there are further analyses you would conduct to help you understand these datasets please include them in your commentary and explain what you would do.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, I approached this task as follows:\n",
    "    \n",
    "1. Explore dataset in Jupyter/Excel\n",
    "2. Use this to educate development of data processing functions in Jupyter\n",
    "3. Set up postgres db in Docker container\n",
    "4. Sketch out an ETL that loads data to postgres\n",
    "5. Develop Dash app in Jupyter environment using JupyterDash\n",
    "6. Set up Dash app in a seperate container that sends query requests to db container\n",
    "\n",
    "The goal was to provide theoretical data consumers with sufficiently processed data (e.g. null values standardised, datetimes converted) to generate more engineering requirements. The features of the data quality dashboard could be used in conjunction with the data itself to educate this process.\n",
    "\n",
    "\n",
    "The database and dashboard are each hosted in docker containers, to easily deploy/scale up in a theoretical production environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import sqlalchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_df = pd.read_csv(\"data/swan_1996_97_baseline.csv\")\n",
    "visit1_df = pd.read_csv(\"data/swan_1997_99_visit1.csv\")\n",
    "visit2_df = pd.read_csv(\"data/swan_1998_00_visit2.csv\")\n",
    "dfs = {'baseline_df': baseline_df, 'visit1_df': visit1_df,'visit2_df': visit2_df}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. First observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How much data are we dealing with?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for name, df in dfs.items():\n",
    "    print(f'{name} dims: {df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  First observations\n",
    "   1. The data is _very_ wide. Due to the width of the data, its easiest digest to interrogate outside of jupyter. Excel will suit fine in this case of csvs on disk. If the location differed, we could choose a different solution (e.g. AWS Athena for S3 data, RDBMS for data in a database, etc.)\n",
    "   2. The number of rows decreases from baseline -> 2nd visit, whether this is due to inherent dropoff in the dataset or a potential issue is unknown at this stage\n",
    "   3. The number of columns decreases similarly.\n",
    "- Action points at this stage after getting eyes on data in excel:\n",
    "    1. Each table has column suffixes which we may or may not want to remove\n",
    "    2. We have a lot of text data that may need encoding into some other form\n",
    "    3. We have a lot of 'NA' strings and other encoding of NULL that may need cleaning\n",
    "    4. There is some time data in string format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Given my current lack of understanding of the data, I will leave multicategorical survey results as is, until a requirement is given to change them. However, we can convert binary text columns to boolean (nullable boolean dtype supported in `pd >= .1.0.0`) and save some memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_bools(df):\n",
    "    \"\"\"\n",
    "    Finds where a dataframe only has binary string values, and converts to boolean.\n",
    "    This ignore\n",
    "    \"\"\"\n",
    "    out_df = df.copy()\n",
    "    for col in df.columns:\n",
    "        if set(df[col][df[col].notna()].unique())  <=  {'(1) No', '(2) Yes' }:\n",
    "             out_df[col] = out_df[col].map({'(1) No': 0, '(2) Yes' :1}).astype(\"boolean\")\n",
    "    return out_df\n",
    "\n",
    "def convert_bools(dict_of_dfs):\n",
    "    \"\"\"\n",
    "    Loops through dataframes and converts strings to booleans where appropriate\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    for name,df in dfs.items():\n",
    "        out[name] =_convert_bools(df)  \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Luckily pandas assumes 'NA' = NaN. But we still have wide variety of string-encoded null data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_whitespace(dict_of_dfs):\n",
    "    \"\"\"\n",
    "    Loops through dataframes and strips whitespace\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    for name,df in dfs.items():\n",
    "        out[name] = _strip_whitespace(df)\n",
    "    return out\n",
    "\n",
    "def _strip_whitespace(df):\n",
    "    \"\"\"\n",
    "    Indexes object columns from a df and strips whitespace\n",
    "    \"\"\"\n",
    "    out_df = df.copy()\n",
    "    for col in out_df.loc[:, out_df.dtypes == object].columns:\n",
    "        out_df[col] = out_df[col].str.strip(' ')\n",
    "    return out_df\n",
    "\n",
    "def clean_nans(dict_of_dfs):\n",
    "    \"\"\"\n",
    "    Loops through dataframes and cleans string-encoded null data \n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    for name,df in dfs.items():\n",
    "        out[name] = _clean_nans(df)\n",
    "    return out\n",
    "\n",
    "def _clean_nans(df):\n",
    "    \"\"\"\n",
    "    Replaces anything in string_nans with na in object columns\n",
    "    \"\"\"\n",
    "    string_nans=['.','', '-1']\n",
    "    replace_dict = dict(zip(string_nans,[np.nan]*3))\n",
    "    out_df = df.copy()\n",
    "    for col in out_df.loc[:, out_df.dtypes == object].columns:\n",
    "        if out_df[col].isin(string_nans).any():\n",
    "            out_df[col] = out_df[col].replace(replace_dict)\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB _clean_nans()_ should be safe for cols like LMPDAY0 which have negative integer values, as they are not read in  as `object` dtype by pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting datetimes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Do we have any datetimes with `/` or `:` formatting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name,df in dfs.items():\n",
    "    for col in df.loc[:, df.dtypes == object].columns:\n",
    "        if df[col].str.contains('/').any():\n",
    "            print(df[col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for name,df in dfs.items():\n",
    "    for col in df.loc[:, df.dtypes == object].columns:\n",
    "        if df[col].str.contains(':').any():\n",
    "            print(f'{name},{col}, {df[col].unique()[:10]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_dt(df_name,col,dict_of_dfs,_format):\n",
    "    \"\"\"\n",
    "    Mutates a col in a dataframe to dt dtype with given format\n",
    "    \"\"\"\n",
    "    dict_of_dfs[df_name][col] = pd.to_datetime(dict_of_dfs[df_name][col],format = _format)\n",
    "    \n",
    "def mutate_dfs_convert_dts(dict_of_dfs):\n",
    "    \"\"\"\n",
    "    Loops through dt specifications and mutates dfs in dict by converting datetimes\n",
    "    \"\"\"\n",
    "    format_1_cols = [\n",
    "    ('visit1_df','STRTIM11'),\n",
    "    ('visit1_df','STPTIM11'),\n",
    "    ('visit2_df','STRTIM12'),\n",
    "    ('visit2_df','STPTIM12'),\n",
    "    ('visit2_df','STRTIM22'),\n",
    "    ('visit2_df','STPTIM22'),\n",
    "    ('visit2_df','STRTIM32'),\n",
    "    ('visit2_df','STPTIM32'),\n",
    "    ]\n",
    "    format_2_cols = [\n",
    "    ('baseline_df','SPSCTIM0'),\n",
    "    ('baseline_df','HPSCTIM0'),\n",
    "    ('visit1_df','SPSCTIM1'),\n",
    "    ('visit1_df','HPSCTIM1'),\n",
    "    ('visit2_df','SPSCTIM2'),\n",
    "    ]\n",
    "\n",
    "    for coltuple in format_1_cols:\n",
    "        _convert_dt(*coltuple,dict_of_dfs,'%H:%M:%S')\n",
    "    for coltuple in format_2_cols:\n",
    "        _convert_dt(*coltuple,dict_of_dfs,'0:%H:%M')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Thinking about data quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4a. Looking at data richness by column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets assume that a column over 97.5% null is less useful for analytics due to a smaller sample size. Quantifying them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nulls_ratios(dict_of_dfs): \n",
    "    \"\"\"\n",
    "    Loops through dict of dfs and returns data on which columns are over 97.5% null in each df\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    for name, df in dfs.items():\n",
    "        nulls = (df.isnull().mean() *100).round(2)\n",
    "        out[name + 'nulls_stats'] = (\n",
    "            pd.DataFrame(\n",
    "                nulls[nulls>=97.5]\n",
    "            )\n",
    "            .reset_index()\n",
    "            .rename(columns = {'index':'colname',0:'percent_null'})\n",
    "        )\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We may want to remove any columns over 97.5 null to reduce the dimensions of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_out_nans_mutates(dict_of_dfs):\n",
    "    \"\"\"\n",
    "    Removes columns with over 97.5% null (yielding a small sample size for analysis given rows per table)\n",
    "    \"\"\"\n",
    "    names = ['baseline_df','visit1_df','visit2_df']\n",
    "    for name in names:\n",
    "        cols_to_drop = dict_of_dfs[name + 'nulls_stats']\n",
    "        cols_to_drop = list(cols_to_drop[cols_to_drop.percent_null > 97.5].colname)\n",
    "        dict_of_dfs[name] = dict_of_dfs[name].drop(columns=cols_to_drop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Quantify how columns are shared between tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initial exploration yielded much smaller intersections in columns than expected- due to suffixes relating to the stage of the study\n",
    "- Accounting for them, how are columns shared between each dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_column_names(dfname, df,df_suffix_dict):\n",
    "    \"\"\"\n",
    "    Removes dataset-specific suffix from colnames of df\n",
    "    \"\"\"\n",
    "    suffix = df_suffix_dict[dfname]\n",
    "    return [re.sub(f'{suffix}$','',col) for col in df.columns]\n",
    "\n",
    "def get_columns_in_a_not_in_b(dict_of_dfs,dfname_a,dfname_b):\n",
    "    \"\"\"\n",
    "    Gets difference of colums in a vs b, accounting for colname suffixes\n",
    "    \"\"\"\n",
    "    df_suffix_dict = {'baseline_df' : '0', 'visit1_df' : '1','visit2_df': '2'}\n",
    "    cleaned_columns_a = get_clean_column_names(dfname_a, dict_of_dfs[dfname_a], df_suffix_dict)\n",
    "    cleaned_columns_b = get_clean_column_names(dfname_b, dict_of_dfs[dfname_b], df_suffix_dict)\n",
    "    set_difference = set(cleaned_columns_a).difference(set(cleaned_columns_b))\n",
    "    out = pd.DataFrame({'columns':[col+df_suffix_dict[dfname_a] for col in set_difference]})\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_columns_in_a_not_in_b(dfs,'visit1_df','visit2_df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This looks more reasonable (in conunction with the documentation provided, I am happy that no info was lost at this stage). Let's see how that compares to baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_columns_in_a_not_in_b(dfs,'visit1_df','baseline_df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Accounting for the suffixes, the baseline table contains a lot of data not in the visit tables, which is not unexpected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Check state of SWANID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First make sure that no SWANID in visit datasets are not present in baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(visit1_df['SWANID']).difference(set(baseline_df['SWANID']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set(visit2_df['SWANID']).difference(set(baseline_df['SWANID']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets also make sure they are always unique per table:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in dfs.items():\n",
    "    print(f'{name}: {df.SWANID.nunique()/df.SWANID.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- See how SWANIDs are conserved throughout the study - how many drop off at each stage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cohort_funnel_stats(dfs):\n",
    "    \"\"\"\n",
    "    Returns data on how many survey subjects are retained at subsequent stages of the study\n",
    "    \"\"\"\n",
    "    total = len(dfs['baseline_df']['SWANID'])\n",
    "    vis1 = len(set(dfs['baseline_df']['SWANID']).intersection(set(dfs['visit1_df']['SWANID'])))\n",
    "    vis2=len(set(dfs['baseline_df']['SWANID']).intersection(set(dfs['visit1_df']['SWANID'])).intersection(set(dfs['visit2_df']['SWANID'])))\n",
    "    cohort = pd.DataFrame({'stage':['baseline','visit1','visit2'], 'subjects':[total,vis1,vis2]})\n",
    "    return cohort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_swanid_index_mutates(dfs):\n",
    "    \"\"\"\n",
    "    Sets pandas index for joining survey datasets if required\n",
    "    \"\"\"\n",
    "    for name in ['baseline_df','visit1_df','visit2_df']:\n",
    "        dfs[name].set_index('SWANID',inplace=True)\n",
    "        \n",
    "def drop_redundant_cols_mutates(dfs):\n",
    "    \"\"\"\n",
    "    Drop cols that are shared between tables in case of join\n",
    "    \"\"\"\n",
    "    drop_dict = {'baseline_df': ['VISIT'],'visit1_df': ['VISIT','RACE'], 'visit2_df': ['VISIT','RACE']}\n",
    "    for dfname, cols in drop_dict.items():\n",
    "        dfs[dfname] = dfs[dfname].drop(columns=cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_visits_mutates(dfs):\n",
    "    \"\"\"\n",
    "    Adds new df - baseline left join subsequent visits - to dict of dfs\n",
    "    \"\"\"\n",
    "    full_data = dfs['baseline_df'].join(dfs['visit1_df'],how = 'left',).join(dfs['visit2_df'], how='left')\n",
    "    dfs['full_data'] = full_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NB - running the above left joins, even after filtering out columns over 97.5% null, exceeds data limits in postgres. \n",
    "- Some kind of data lake (e.g. s3) or data warehouse (e.g. Redshift) might be preferable to store this joined data in reality. \n",
    "- Sticking to the limits imposed by my Postgres container solution, I will load the tables seperately into the database. However, I will create indexes on SWANID to speed up any joining required by data consumers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we have some data processing functions, we ideally want 100% unit test coverage of them. \n",
    "- Given the confines of this task, and the fact that we are developing in Jupyter, let's sketch out what some of our unit tests might look like (when included in a pytest testing suite)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_strip_whitespace():\n",
    "    _in = pd.DataFrame({\n",
    "        'col1':[' one', '   . ', 'two', ' '],\n",
    "        'col2':[1,2,3,4]\n",
    "    })\n",
    "    target = pd.DataFrame({\n",
    "        'col1':['one', '.', 'two', ''],\n",
    "        'col2':[1,2,3,4]\n",
    "    })\n",
    "    out = _strip_whitespace(_in)\n",
    "    pd.testing.assert_frame_equal(out,target)\n",
    "\n",
    "def test_clean_nans():\n",
    "    _in = pd.DataFrame({\n",
    "        'col1':['one','-1','two', ''],\n",
    "        'col2':['three','-1','four', '.'],\n",
    "    })\n",
    "    target = pd.DataFrame({\n",
    "        'col1':['one',np.nan,'two', np.nan],\n",
    "        'col2':['three',np.nan,'four', np.nan],\n",
    "    })\n",
    "    out = _clean_nans(_in)\n",
    "    pd.testing.assert_frame_equal(out,target)\n",
    "\n",
    "def test_convert_bools():\n",
    "    \"\"\"\n",
    "    NB -calls to convert dtypes() are to convert from np bool (default) to pandas boolean dtype\n",
    "    \"\"\"\n",
    "    _in = pd.DataFrame({\n",
    "        'col1':['(1) No', '(2) Yes', '(2) Yes', np.nan],\n",
    "        'col3':['(1) No', '(2) Yes', '(2) Yes', '(2) Yes'],\n",
    "        'col2':['three','-1','four', '.'],\n",
    "    })\n",
    "    target = pd.DataFrame({\n",
    "        'col1':pd.Series([False, True, True, np.nan]).convert_dtypes(),\n",
    "        'col3':pd.Series([False, True, True, True]).convert_dtypes(),\n",
    "        'col2':['three','-1','four', '.'],\n",
    "    })\n",
    "    out = _convert_bools(_in)\n",
    "    print(out)\n",
    "    pd.testing.assert_frame_equal(out,target)\n",
    "    \n",
    "test_strip_whitespace()\n",
    "test_clean_nans()\n",
    "test_convert_bools()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data aggregation and presentation\n",
    "\n",
    "Now you have a basic understanding of the data, the next step is to **build a simple 1 page dashboard that summarises the key characteristics of the datasets**.  \n",
    "\n",
    "For this task, you will need to consider the following areas: \n",
    "- How to aggregate the 3 datasets into one data structure (e.g. a database) that can be queried (with your chosen programming language) \n",
    "- What exhibits to display\n",
    "- How to transform the data for the selected exhibits\n",
    "- How to host the dashboard (note: the dashboard does not require a public URL, you can demo on your local machine)\n",
    "\n",
    "##### To help simplify this task, please use the following guidelines:\n",
    "- Stick to a 1 page layout and do not try and prepare more than 6 exhibits \n",
    "- Don't worry about 'perfect' styling - we understand this can take a lot of time and we're most interested in your overall approach and the core components of your implementation \n",
    "\n",
    "Your implementation will likely require writing code outside of the Jupyter notebook environment.  \n",
    "So, in this notebook please just provide a summary of your approach and add written detail for what else you would do that you haven't included.    \n",
    "As in step 1, in your written code, please include inline code comments or markdown to explain your approach.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using the data processing functions developed through exploration, we have something that looks like an ETL pipeline\n",
    "- Data are read from csv on disk, processed, and loaded to the postgres database running on the db service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_df = pd.read_csv(\"data/swan_1996_97_baseline.csv\")\n",
    "visit1_df = pd.read_csv(\"data/swan_1997_99_visit1.csv\")\n",
    "visit2_df = pd.read_csv(\"data/swan_1998_00_visit2.csv\")\n",
    "dfs = {'baseline_df': baseline_df, 'visit1_df': visit1_df,'visit2_df': visit2_df}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = convert_bools(dfs)\n",
    "dfs = strip_whitespace(dfs)\n",
    "dfs = clean_nans(dfs)\n",
    "dfs = {**dfs,**get_nulls_ratios(dfs)}\n",
    "mutate_dfs_convert_dts(dfs)\n",
    "drop_redundant_cols_mutates(dfs)\n",
    "dfs = {**dfs, **{'cols_in_visit1_not_in_baseline':get_columns_in_a_not_in_b(dfs,'visit1_df','baseline_df')}}\n",
    "dfs = {**dfs, **{'cols_in_visit2_not_in_visit1':get_columns_in_a_not_in_b(dfs,'visit2_df','visit1_df')}}\n",
    "dfs = {**dfs, **{'cohort_funnel_stats': get_cohort_funnel_stats(dfs)}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('postgresql://postgres:postgres@127.0.0.1:5433/postgres')\n",
    "for name, df in dfs.items():\n",
    "    df.to_sql(name,engine,if_exists ='replace',index=False)\n",
    "for table in ['baseline_df','visit1_df','visit2_df']:\n",
    "    with engine.connect() as con:\n",
    "        con.execute(f'ALTER TABLE {table} ADD PRIMARY KEY (\"SWANID\");')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion/Potential Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ideally, a full docker network including a container running jupyter would have been more clean, but the extra config required did not fit the time constraints of the task. However, the dashboard service and persistent db service are a good start towards generalisation to other local machines or remote deployment\n",
    "- For a postgres server a mounted volume on the host can be desirable, but was not necessary for the sake of this task.\n",
    "- As mentioned above, an alternative datastore solution (e.g. data lake or columnar datastore) would allow us to store the joined data, and move query-writing workload away from data consumers. My assumption is that data consumers e.g. data scientists would want the data from each survey assimilated into a wide format for model building. As a short term solution, ensuring the seperate tables have indices on SWANID will reduce querying time for data consumers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I chose to use `dash`, which in my experience is the fastest way to build dashboards in pure python. As a long-term solution for visualising the data in question, it is sub-optimal as more python development is required to add more features. \n",
    "- A comprehensive open source BI tool such as Redash would have been my ideal solution, as it allows data consumers to create their own dashboards with SQL. However setup of a Redash service is outside the time constraints of this task.\n",
    "- A piece of documentation explaining the dashboard elements would also be best practice, but I did not have time to get to this in this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Within the time constraints of this task, I prioritised exploring aspects of data quality over dataset-specific insights and requirements. This would be a reasonable expectation over a longer time frame, ideally working closely with colleagues with domain-specific knowledge of this dataset. \n",
    "- However, my solution provides the first step in this process of iterative requirement creation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As highlighted in Section 1, more thorough test coverage would be essential as part of transitioning this notebook code into a proper ETL codebase\n",
    "\n",
    "- Encoding of text-based categorical survey responses could have saved memory, but I operated under the assumption that it is better to leave the data consumers to make this requirement if they wish. \n",
    "\n",
    "- Similarly, removing columns with high % null is something I accounted for as a potentially useful requirement, but one that would be decided on in conjunction with data consumers. \n",
    "\n",
    "- As with all data processing, we would ideally ensure that all information governance/GDPR conditions are adhered to."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-venv",
   "language": "python",
   "name": "local-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
